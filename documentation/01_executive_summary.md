# Executive Summary (Glue Version)

This is is AWS hosted and glue run pipeline. Use ETL pipeline. It validates events, transactions and users datasets files during ingestion and standardizes mutlti-format(csv, .csv.gz .json, .ndjson, .ndjson.gz, .txt, .gz) and quarantine corrupt files. It transforms 9.1 GB of highly messy ecommrce data into analytics data. The entire pipeline is automated using Airflow. It starts with event-driven raw ingestion, the Master orchestrator dag triggers AWS GLUE SPARK jobs for large scale transformation of bronze > silver > gold, and then validates s3 output before loading into Redshift serverless using COPY and PARQUET format. 
The data pipeline is designed to support data analysts, business intelligence teams, and downstream consumers by ensuring that behavioral events, users records and transactions data are processed with fixed schema enforcement, schema drifts free, data quality validation. and bad files and records are quarantine for further investigation.
SCD user history based on timestamp and has no tie breaker due to lack of column but in real production work, ingestion timestamp is must if such a situation arise. 
This data pipeline ensures realiability and scalability through clear seperation of responsibiliy: Airflow manages orchestration and dependency control, Glue Spark jobs handle all transformation and modeling logic while Redhshifte serverless provies a scalable low-maintenance analytics warehouse. So as result, gold-layer's dimension, facts and marts are rebuilt per run allowing bi teams to build dashborads, run ad-hoc quries and perform business analysis. 
